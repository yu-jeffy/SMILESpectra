{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768307a0-3077-4acc-9def-0b044e54990c",
   "metadata": {},
   "source": [
    "Install prereqs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99bfe2eb-be20-4b3f-b682-eb663d8db145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T05:59:55.935928Z",
     "iopub.status.busy": "2023-12-05T05:59:55.935663Z",
     "iopub.status.idle": "2023-12-05T06:00:01.535368Z",
     "shell.execute_reply": "2023-12-05T06:00:01.534565Z",
     "shell.execute_reply.started": "2023-12-05T05:59:55.935907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.23.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: SmilesPE in /usr/local/lib/python3.9/dist-packages (0.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from SmilesPE) (4.3.2)\n",
      "Requirement already satisfied: fastprogress in /usr/local/lib/python3.9/dist-packages (from SmilesPE) (1.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (1.9.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (6.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: SmilesPE in /usr/local/lib/python3.9/dist-packages (0.0.3)\n",
      "Requirement already satisfied: fastprogress in /usr/local/lib/python3.9/dist-packages (from SmilesPE) (1.0.3)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from SmilesPE) (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (1.9.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (6.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch torchvision SmilesPE\n",
    "\n",
    "# Install Tokenizer\n",
    "!pip install SmilesPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d156b-440f-4881-9061-d17f632dfe6b",
   "metadata": {},
   "source": [
    "Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31de9cb5-620a-4f06-8ca4-2aa35335a366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:01.537025Z",
     "iopub.status.busy": "2023-12-05T06:00:01.536815Z",
     "iopub.status.idle": "2023-12-05T06:00:01.541268Z",
     "shell.execute_reply": "2023-12-05T06:00:01.540682Z",
     "shell.execute_reply.started": "2023-12-05T06:00:01.537006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# memory allocation fix\n",
    "# import os\n",
    "\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e710c0-a567-4657-821f-74d458f7c150",
   "metadata": {},
   "source": [
    "Load in datasets for training and testing. Using ZINC Drugs Clean, 1 million randomly sampled, for training. Using Massbank MS Positive for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "179afcbf-5079-4bb1-b81d-f208406fd57f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:01.542027Z",
     "iopub.status.busy": "2023-12-05T06:00:01.541834Z",
     "iopub.status.idle": "2023-12-05T06:00:05.704874Z",
     "shell.execute_reply": "2023-12-05T06:00:05.704221Z",
     "shell.execute_reply.started": "2023-12-05T06:00:01.542009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Define the function to divide DataFrame into chunks\n",
    "def divide_into_chunks(df, chunk_size):\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Load the full training data\n",
    "raw_training = pd.read_csv('zinc_training.csv', skiprows=[724671])\n",
    "\n",
    "# Randomly sample entries from the raw_training DataFrame\n",
    "train_sample = raw_training.sample(n=100, random_state=42)\n",
    "\n",
    "# Clear the memory occupied by raw_training\n",
    "del raw_training\n",
    "gc.collect()\n",
    "\n",
    "# Create chunks from train_chunk\n",
    "train_chunks = divide_into_chunks(train_sample, chunk_size=100)\n",
    "\n",
    "raw_testing = pd.read_csv('massbank_testing.csv', skiprows=[410])\n",
    "\n",
    "test_sample = raw_testing.sample(n=100, random_state=42)\n",
    "\n",
    "# Clear the memory occupied by raw_training\n",
    "del raw_testing\n",
    "gc.collect()\n",
    "\n",
    "test_chunks = divide_into_chunks(test_sample, chunk_size=100)\n",
    "\n",
    "print(\"Done! ðŸ™Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b2370-4ee3-4549-97f6-27870952f847",
   "metadata": {},
   "source": [
    "Data Processing\n",
    "\n",
    "Preprocessing the SMILES data, using SMILES Pair Encoding. Link to encoding library and vocabulary list: https://github.com/XinhaoLi74/SmilesPE\n",
    "\n",
    "Preprocessing the mass spectra data using binning procedure, categorizing each peak into fixed bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f9e583-a3b2-44b9-bd5b-6c044526fbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:05.706859Z",
     "iopub.status.busy": "2023-12-05T06:00:05.706640Z",
     "iopub.status.idle": "2023-12-05T06:00:05.968756Z",
     "shell.execute_reply": "2023-12-05T06:00:05.968133Z",
     "shell.execute_reply.started": "2023-12-05T06:00:05.706840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized SMILES (Training): Cc1n nc2n1 CCC[C@H]2 NC(=O) c3cc( cn 3C) C#N\n",
      "Encoded Spectra (Training): [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "Tokenized SMILES (Testing): c( c2)ccc( c2) NC(=O) C(= C(C) 1) S(=O)(=O) CCO1\n",
      "Encoded Spectra (Testing): [0. 0. 0. ... 0. 0. 0.]\n",
      "Done! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from SmilesPE.tokenizer import SPE_Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Path to your SPE vocabulary file\n",
    "spe_vob_path = 'SPE_ChEMBL.txt'\n",
    "\n",
    "# Open the SPE vocabulary file and initialize the tokenizer\n",
    "with codecs.open(spe_vob_path, encoding='utf-8') as spe_vob:\n",
    "    spe = SPE_Tokenizer(spe_vob)\n",
    "\n",
    "# Function for encoding SMILES data\n",
    "def preprocess_smiles(smiles_data, tokenizer):\n",
    "    return smiles_data.apply(tokenizer.tokenize)\n",
    "    \n",
    "# Function for encoding spectra data\n",
    "def encode_mass_spectra(spectra, max_mz=500, bin_precision=0.01, max_peaks=100):\n",
    "    if isinstance(spectra, str):\n",
    "        spectra = np.array([float(x) for x in spectra.strip('[]').split(', ') if float(x) <= max_mz])\n",
    "    elif isinstance(spectra, list):\n",
    "        spectra = np.array([x for x in spectra if x <= max_mz])\n",
    "    else:\n",
    "        raise ValueError(\"Spectra data must be either a string or a list of floats\")\n",
    "\n",
    "    if len(spectra) > max_peaks:\n",
    "        spectra = spectra[:max_peaks]\n",
    "\n",
    "    bin_indices = np.floor(spectra / bin_precision).astype(int)\n",
    "    encoded = np.zeros(int(max_mz / bin_precision))\n",
    "    encoded[bin_indices] = 1\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Function for processing each chunk, with garbage collection\n",
    "def process_chunks(chunks, smiles_col, spectra_col, tokenizer):\n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # Create a copy of the chunk to avoid SettingWithCopyWarning\n",
    "        working_chunk = chunk.copy()\n",
    "\n",
    "        # Process the data\n",
    "        working_chunk['tokenized_smiles'] = preprocess_smiles(working_chunk[smiles_col], tokenizer)\n",
    "        working_chunk['encoded_spectra'] = working_chunk[spectra_col].apply(encode_mass_spectra)\n",
    "\n",
    "        # Append the processed chunk to the list\n",
    "        processed_chunks.append(working_chunk)\n",
    "\n",
    "        # Free up memory\n",
    "        del working_chunk\n",
    "        gc.collect()\n",
    "        # torch.cuda.empty_cache()  # Optionally clear unused GPU memory after processing\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "# Process training and testing data\n",
    "processed_train_chunks = process_chunks(train_chunks, 'smiles', 'METFRAG_MZ', spe)\n",
    "processed_test_chunks = process_chunks(test_chunks, 'smiles', 'spectrum', spe)\n",
    "\n",
    "# Inspect tokenized SMILES and encoded spectra from the first row of the first training chunk\n",
    "first_train_row = processed_train_chunks[0].iloc[0]\n",
    "print(\"Tokenized SMILES (Training):\", first_train_row['tokenized_smiles'])\n",
    "print(\"Encoded Spectra (Training):\", first_train_row['encoded_spectra'])\n",
    "\n",
    "# Similar inspection for the testing data\n",
    "first_test_row = processed_test_chunks[0].iloc[0]\n",
    "print(\"\\nTokenized SMILES (Testing):\", first_test_row['tokenized_smiles'])\n",
    "print(\"Encoded Spectra (Testing):\", first_test_row['encoded_spectra'])\n",
    "\n",
    "\n",
    "print(\"Done! ðŸ™Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa1efd8d-d735-45ff-8e18-9de3d07713c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:05.969780Z",
     "iopub.status.busy": "2023-12-05T06:00:05.969586Z",
     "iopub.status.idle": "2023-12-05T06:00:05.977640Z",
     "shell.execute_reply": "2023-12-05T06:00:05.977037Z",
     "shell.execute_reply.started": "2023-12-05T06:00:05.969755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for encoding spectra data, vectorized with numpy\n",
    "def encode_mass_spectra_vectorized(spectra, max_mz=500, bin_precision=0.01, max_peaks=100):\n",
    "    # Handle both string and list input\n",
    "    if isinstance(spectra, str):\n",
    "        spectra = np.array([float(x) for x in spectra.strip('[]').split(', ') if float(x) <= max_mz])\n",
    "    elif isinstance(spectra, list):\n",
    "        spectra = np.array(spectra)\n",
    "    else:\n",
    "        raise ValueError(\"Spectra data must be either a string or a list of floats\")\n",
    "\n",
    "    # Ensure spectra are within the specified m/z range\n",
    "    spectra = spectra[spectra <= max_mz]\n",
    "\n",
    "    # Limit the number of peaks\n",
    "    if len(spectra) > max_peaks:\n",
    "        # Sort spectra and select the highest intensity peaks (assuming higher intensity means more important)\n",
    "        spectra = np.sort(spectra)[-max_peaks:]\n",
    "\n",
    "    # Binning procedure\n",
    "    bin_indices = np.floor(spectra / bin_precision).astype(int)\n",
    "    bin_indices = bin_indices[bin_indices < int(max_mz / bin_precision)] # Ensure indices are within range\n",
    "\n",
    "    # Create a one-hot encoded vector for each peak\n",
    "    encoded = np.zeros(int(max_mz / bin_precision))\n",
    "    np.put(encoded, bin_indices, 1)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "# Function for encoding spectra data, using GPU\n",
    "def encode_mass_spectra_gpu(spectra, max_mz=500, bin_precision=0.05, max_peaks=50, device='cuda'):\n",
    "    if isinstance(spectra, str):\n",
    "        spectra = [float(x) for x in spectra.strip('[]').split(', ') if float(x) <= max_mz]\n",
    "    elif not isinstance(spectra, list):\n",
    "        raise ValueError(\"Spectra data must be either a string or a list of floats\")\n",
    "\n",
    "    # Convert to tensor and use half-precision floats\n",
    "    spectra = torch.tensor(spectra, device=device, dtype=torch.float16)\n",
    "\n",
    "    spectra = spectra[spectra <= max_mz]\n",
    "\n",
    "    if spectra.size(0) > max_peaks:\n",
    "        spectra, _ = torch.sort(spectra, descending=True)\n",
    "        spectra = spectra[:max_peaks]\n",
    "\n",
    "    bin_indices = torch.floor(spectra / bin_precision).type(torch.int64)  # int64 for indices\n",
    "    bin_indices = bin_indices[bin_indices < int(max_mz / bin_precision)]\n",
    "\n",
    "    # Use a half-precision float tensor for the encoded array\n",
    "    encoded = torch.zeros(int(max_mz / bin_precision), device=device, dtype=torch.float16)\n",
    "    encoded.scatter_(0, bin_indices, 1)\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e896a-df7c-4a96-9a95-f7b9e70217c3",
   "metadata": {},
   "source": [
    "Concatenate Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3533ab4-6e0e-41e9-86b1-f72f1181f759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:05.978641Z",
     "iopub.status.busy": "2023-12-05T06:00:05.978457Z",
     "iopub.status.idle": "2023-12-05T06:00:05.989045Z",
     "shell.execute_reply": "2023-12-05T06:00:05.988468Z",
     "shell.execute_reply.started": "2023-12-05T06:00:05.978618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               smiles  \\\n",
      "0               Cc1nnc2n1CCC[C@H]2NC(=O)c3cc(cn3C)C#N   \n",
      "1   Cc1cc(sc1C(=O)N2CCC[C@@H](C2)NS(=O)(=O)C)NC(=O...   \n",
      "2   Cc1c(nc(s1)NC(=O)C[C@H]2c3ccccc3C(=O)O2)c4ccc(...   \n",
      "3   COc1ccc2c(c1)CCCN2C(=O)c3ccc(cc3)CS(=O)(=O)c4c...   \n",
      "4     Cc1ccccc1Cc2nc(no2)C[NH+]3CCC4(CC3)OC[C@H](O4)C   \n",
      "..                                                ...   \n",
      "95  Cc1ccc(c(c1)N(C(C)C)C(=O)[C@@H]2Cc3ccccc3N2C(=...   \n",
      "96       CCc1nnc(s1)NC(=O)[C@H]2[C@@H](C2(C)C)C=C(C)C   \n",
      "97    Cc1c(cccc1Cl)NC(=O)CN(c2cc(ccc2OC)Cl)S(=O)(=O)C   \n",
      "98  CCCc1cc(n(n1)C)C(=O)N2CC[C@@H]3CC[C@H](C2)[NH+]3C   \n",
      "99                           Cc1c(cnn1C)CNc2cccc(c2)I   \n",
      "\n",
      "                                           METFRAG_MZ  \\\n",
      "0   [81.03215, 85.05224, 91.02907, 93.02091, 95.04...   \n",
      "1   [81.987175, 84.08082, 85.052246, 85.08865, 86....   \n",
      "2   [82.98242, 87.03149, 90.046425, 93.03351, 97.9...   \n",
      "3   [85.05224, 90.04643, 91.05426, 92.02567, 104.0...   \n",
      "4   [83.02399, 83.023994, 84.080826, 86.03625, 86....   \n",
      "..                                                ...   \n",
      "95  [85.05224, 86.06007, 91.041664, 91.05425, 99.0...   \n",
      "96  [82.041336, 82.07775, 84.057, 84.98549, 94.077...   \n",
      "97  [91.041664, 92.02567, 92.9879, 93.99573, 107.0...   \n",
      "98  [81.04474, 84.08082, 84.080826, 85.052246, 85....   \n",
      "99  [92.04949, 95.04781, 106.06515, 171.07913, 313...   \n",
      "\n",
      "                                     tokenized_smiles  \\\n",
      "0        Cc1n nc2n1 CCC[C@H]2 NC(=O) c3cc( cn 3C) C#N   \n",
      "1   Cc1cc( sc1 C(=O)N2 CCC[C@@H]( C2) NS(=O)(=O) C...   \n",
      "2   Cc1c( nc( s1) NC(=O) C[C@H]2 c3ccccc3 C(=O)O 2...   \n",
      "3   COc1ccc2c(c1) CCCN2C(=O) c3ccc( cc3) CS(=O)(=O...   \n",
      "4   Cc1ccccc1 C c2nc( no2) C [NH+] 3 CCC4 ( CC3) O...   \n",
      "..                                                ...   \n",
      "95  Cc1ccc( c( c1) N(C(C)C) C(=O) [C@@H]2 Cc3ccccc...   \n",
      "96  CCc1n nc( s1) NC(=O) [C@H]2[C@@H]( C2(C)C) C= ...   \n",
      "97  Cc1c( cccc1 Cl) NC(=O) CN( c2cc( ccc2 OC) Cl) ...   \n",
      "98  CCC c1cc( n( n1) C) C(=O)N2 CC[C@@H]3 CC[C@H](...   \n",
      "99                     Cc1c( cnn1 C) CN c2cccc( c2) I   \n",
      "\n",
      "                                      encoded_spectra  \n",
      "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "..                                                ...  \n",
      "95  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "96  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "97  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "98  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "99  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "\n",
      "[100 rows x 4 columns]\n",
      "Done! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "# Concatenate processed training data chunks\n",
    "processed_train_df = pd.concat(processed_train_chunks, ignore_index=True)\n",
    "\n",
    "# Concatenate processed testing data chunks\n",
    "processed_test_df = pd.concat(processed_test_chunks, ignore_index=True)\n",
    "\n",
    "print(processed_train_df)\n",
    "\n",
    "print(\"Done! ðŸ™Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a6f75-caac-4c31-b383-289e38e4bf06",
   "metadata": {},
   "source": [
    "Tranformer and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb1056c5-2134-492a-98de-137622893faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:05.990045Z",
     "iopub.status.busy": "2023-12-05T06:00:05.989856Z",
     "iopub.status.idle": "2023-12-05T06:00:06.411201Z",
     "shell.execute_reply": "2023-12-05T06:00:06.410449Z",
     "shell.execute_reply.started": "2023-12-05T06:00:05.990027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ðŸ› \n"
     ]
    }
   ],
   "source": [
    "# Load the SPE vocabulary file and create a mapping from tokens to IDs\n",
    "spe_vob_path = 'SPE_ChEMBL.txt'\n",
    "with open(spe_vob_path, 'r', encoding='utf-8') as file:\n",
    "    tokens = [line.strip() for line in file.readlines()]\n",
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "\n",
    "# Add a padding token to the token_to_id dictionary\n",
    "pad_token = '<PAD>'\n",
    "if pad_token not in token_to_id:\n",
    "    token_to_id[pad_token] = len(token_to_id)\n",
    "pad_token_id = token_to_id[pad_token]\n",
    "\n",
    "# Function to convert tokenized SMILES to IDs\n",
    "def smiles_to_ids(tokenized_smiles, token_to_id):\n",
    "    return [token_to_id[token] if token in token_to_id else token_to_id['<UNK>'] for token in tokenized_smiles]\n",
    "\n",
    "# Add an unknown token to the token_to_id dictionary if it doesn't exist\n",
    "if '<UNK>' not in token_to_id:\n",
    "    token_to_id['<UNK>'] = len(token_to_id)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 512  # The number of expected features in the encoder/decoder inputs\n",
    "nhead = 8  # The number of heads in the multiheadattention models\n",
    "num_encoder_layers = 3  # The number of sub-encoder-layers in the encoder\n",
    "num_decoder_layers = 3  # The number of sub-decoder-layers in the decoder\n",
    "dim_feedforward = 2048  # The dimension of the feedforward network model\n",
    "dropout = 0.1  # The dropout value\n",
    "batch_size = 64  # Batch size for training\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(num_tokens, d_model, padding_idx=pad_token_id)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.decoder_embedding = nn.Embedding(num_tokens, d_model, padding_idx=pad_token_id)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        src = self.encoder_embedding(src) * math.sqrt(d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.decoder_embedding(tgt) * math.sqrt(d_model)\n",
    "        tgt = self.pos_decoder(tgt)\n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "\n",
    "# Initialize the transformer model\n",
    "# num_tokens = 3002 + 1 +1  # Plus one for the padding token\n",
    "num_tokens = len(token_to_id)\n",
    "transformer_model = TransformerModel(num_tokens, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Initialized ðŸ› \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea94863-344f-45a4-86d6-8c6e65411627",
   "metadata": {},
   "source": [
    "Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5dfcd58-13a1-4421-ad06-0c2240461fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:06.412803Z",
     "iopub.status.busy": "2023-12-05T06:00:06.412092Z",
     "iopub.status.idle": "2023-12-05T06:00:06.420418Z",
     "shell.execute_reply": "2023-12-05T06:00:06.419945Z",
     "shell.execute_reply.started": "2023-12-05T06:00:06.412777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "class SMILESToSpectraDataset(Dataset):\n",
    "    def __init__(self, smiles, spectra, token_to_id, max_length=128):\n",
    "        self.smiles = smiles\n",
    "        self.spectra = spectra\n",
    "        self.token_to_id = token_to_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the SMILES string and convert to IDs\n",
    "        tokenized_smiles = spe.tokenize(self.smiles[idx]).split()\n",
    "        smiles_ids = smiles_to_ids(tokenized_smiles, self.token_to_id)\n",
    "        encoded_spectra = self.spectra[idx]\n",
    "        \n",
    "        # Calculate padding lengths, ensuring non-negative values\n",
    "        smiles_padding_length = max(self.max_length - len(smiles_ids), 0)\n",
    "        spectra_padding_length = max(self.max_length - len(encoded_spectra), 0)\n",
    "        \n",
    "        # Padding with the ID of the pad_token\n",
    "        padded_smiles_ids = np.pad(smiles_ids, (0, smiles_padding_length), mode='constant', constant_values=pad_token_id)\n",
    "        padded_spectra = np.pad(encoded_spectra, (0, spectra_padding_length), mode='constant', constant_values=0)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        src = torch.tensor(padded_smiles_ids, dtype=torch.long)\n",
    "        tgt = torch.tensor(padded_spectra, dtype=torch.float)\n",
    "        \n",
    "        return src, tgt\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SMILESToSpectraDataset(processed_train_df['smiles'].tolist(), processed_train_df['encoded_spectra'].tolist(), token_to_id)\n",
    "test_dataset = SMILESToSpectraDataset(processed_test_df['smiles'].tolist(), processed_test_df['encoded_spectra'].tolist(), token_to_id)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(\"Done ðŸ™Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76737f79-82a5-4de2-a50c-98084a353826",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e79145-d4f5-4fe2-b016-e8199a4a3e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T06:00:06.421962Z",
     "iopub.status.busy": "2023-12-05T06:00:06.421433Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_padding_mask(seq, pad_token_id):\n",
    "    return (seq == pad_token_id).transpose(0, 1)\n",
    "\n",
    "# Example usage:\n",
    "# src_mask = generate_square_subsequent_mask(src_seq_length)\n",
    "# tgt_mask = generate_square_subsequent_mask(tgt_seq_length)\n",
    "# src_padding_mask = create_padding_mask(src, pad_token_id)\n",
    "# tgt_padding_mask = create_padding_mask(tgt, pad_token_id)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(transformer_model.parameters())\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, train_loader, criterion, optimizer, pad_token_id):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src_seq_length, tgt_seq_length = src.size(1), tgt.size(1)\n",
    "        \n",
    "        # Create masks\n",
    "        src_mask = generate_square_subsequent_mask(src_seq_length).to(src.device)\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_seq_length).to(tgt.device)\n",
    "        src_padding_mask = create_padding_mask(src, pad_token_id).to(src.device)\n",
    "        tgt_padding_mask = create_padding_mask(tgt, pad_token_id).to(tgt.device)\n",
    "        memory_key_padding_mask = src_padding_mask.clone()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass the masks to the forward method\n",
    "        output = model(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        # Compute the loss; we may need to adjust the target to match the output shape\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(transformer_model, train_loader, criterion, optimizer, pad_token_id)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed814156-6ad5-48a6-a47d-40935a3134e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in test_loader:\n",
    "            output = model(src, tgt)\n",
    "            loss = criterion(output, tgt)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = evaluate(transformer_model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c90f42-7c7b-43a8-9458-cc6346b602de",
   "metadata": {},
   "source": [
    "View model's output on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df74f17-5380-4b32-8815-c3cc8fd28811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to convert logits to binary predictions\n",
    "def logits_to_binary_predictions(logits, threshold=0.5):\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    binary_predictions = (probabilities >= threshold).int()\n",
    "    return binary_predictions\n",
    "\n",
    "# Function to print model predictions and actual values\n",
    "def print_model_predictions(model, data_loader, tokenizer, num_samples=5):\n",
    "    model.eval()\n",
    "    samples_printed = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            output = model(src, tgt)\n",
    "            binary_predictions = logits_to_binary_predictions(output)\n",
    "            \n",
    "            # Convert tokenized SMILES back to string\n",
    "            smiles = [''.join(tokenizer.decode(token)) for token in src]\n",
    "            \n",
    "            # Print the results\n",
    "            for i in range(src.size(0)):\n",
    "                if samples_printed >= num_samples:\n",
    "                    break\n",
    "                print(f\"SMILES: {smiles[i]}\")\n",
    "                print(f\"Predicted Spectra: {binary_predictions[i].tolist()}\")\n",
    "                print(f\"Actual Spectra: {tgt[i].tolist()}\\n\")\n",
    "                samples_printed += 1\n",
    "            if samples_printed >= num_samples:\n",
    "                break\n",
    "\n",
    "# Print model predictions on the validation set\n",
    "print_model_predictions(transformer_model, test_loader, spe, num_samples=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

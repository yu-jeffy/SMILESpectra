{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768307a0-3077-4acc-9def-0b044e54990c",
   "metadata": {},
   "source": [
    "Install prereqs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bfe2eb-be20-4b3f-b682-eb663d8db145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:30:41.034718Z",
     "iopub.status.busy": "2023-11-27T21:30:41.034045Z",
     "iopub.status.idle": "2023-11-27T21:30:47.880981Z",
     "shell.execute_reply": "2023-11-27T21:30:47.880042Z",
     "shell.execute_reply.started": "2023-11-27T21:30:41.034684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.23.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: SmilesPE in /usr/local/lib/python3.9/dist-packages (0.0.3)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from SmilesPE) (4.3.2)\n",
      "Requirement already satisfied: fastprogress in /usr/local/lib/python3.9/dist-packages (from SmilesPE) (1.0.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (1.9.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim->SmilesPE) (1.23.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch torchvision\n",
    "\n",
    "# Install Tokenizer\n",
    "!pip install SmilesPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d156b-440f-4881-9061-d17f632dfe6b",
   "metadata": {},
   "source": [
    "Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31de9cb5-620a-4f06-8ca4-2aa35335a366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:30:47.883264Z",
     "iopub.status.busy": "2023-11-27T21:30:47.882698Z",
     "iopub.status.idle": "2023-11-27T21:30:49.320230Z",
     "shell.execute_reply": "2023-11-27T21:30:49.318920Z",
     "shell.execute_reply.started": "2023-11-27T21:30:47.883232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# memory allocation fix\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e710c0-a567-4657-821f-74d458f7c150",
   "metadata": {},
   "source": [
    "Load in datasets for training and testing. Using ZINC Drugs Clean, 1 million randomly sampled, for training. Using Massbank MS Positive for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179afcbf-5079-4bb1-b81d-f208406fd57f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:30:49.322085Z",
     "iopub.status.busy": "2023-11-27T21:30:49.321759Z",
     "iopub.status.idle": "2023-11-27T21:30:54.837391Z",
     "shell.execute_reply": "2023-11-27T21:30:54.836648Z",
     "shell.execute_reply.started": "2023-11-27T21:30:49.322060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Define the function to divide DataFrame into chunks\n",
    "def divide_into_chunks(df, chunk_size):\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Load the full training data\n",
    "raw_training = pd.read_csv('zinc_training.csv', skiprows=[724671])\n",
    "\n",
    "# Randomly sample 50,000 entries from the raw_training DataFrame\n",
    "train_chunk = raw_training.sample(n=50000, random_state=42)\n",
    "\n",
    "# Clear the memory occupied by raw_training\n",
    "del raw_training\n",
    "gc.collect()\n",
    "\n",
    "# Create chunks from train_chunk\n",
    "train_chunks = divide_into_chunks(train_chunk, chunk_size=100)\n",
    "\n",
    "# Load testing data in chunks\n",
    "test_chunks = pd.read_csv('massbank_testing.csv', skiprows=[410], chunksize=100)\n",
    "\n",
    "print(\"Done! ðŸ™Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b2370-4ee3-4549-97f6-27870952f847",
   "metadata": {},
   "source": [
    "Data Processing\n",
    "\n",
    "Preprocessing the SMILES data, using SMILES Pair Encoding. Link to encoding library and vocabulary list: https://github.com/XinhaoLi74/SmilesPE\n",
    "\n",
    "Preprocessing the mass spectra data using binning procedure, categorizing each peak into fixed bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f9e583-a3b2-44b9-bd5b-6c044526fbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:30:54.840120Z",
     "iopub.status.busy": "2023-11-27T21:30:54.839861Z",
     "iopub.status.idle": "2023-11-27T21:32:11.198166Z",
     "shell.execute_reply": "2023-11-27T21:32:11.197082Z",
     "shell.execute_reply.started": "2023-11-27T21:30:54.840095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from SmilesPE.tokenizer import SPE_Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Path to your SPE vocabulary file\n",
    "spe_vob_path = 'SPE_ChEMBL.txt'\n",
    "\n",
    "# Open the SPE vocabulary file and initialize the tokenizer\n",
    "with codecs.open(spe_vob_path, encoding='utf-8') as spe_vob:\n",
    "    spe = SPE_Tokenizer(spe_vob)\n",
    "\n",
    "# Function for encoding SMILES data\n",
    "def preprocess_smiles(smiles_data, tokenizer):\n",
    "    return smiles_data.apply(tokenizer.tokenize)\n",
    "    \n",
    "# Function for encoding spectra data\n",
    "def encode_mass_spectra(spectra, max_mz=500, bin_precision=0.01, max_peaks=100):\n",
    "    if isinstance(spectra, str):\n",
    "        spectra = np.array([float(x) for x in spectra.strip('[]').split(', ') if float(x) <= max_mz])\n",
    "    elif isinstance(spectra, list):\n",
    "        spectra = np.array([x for x in spectra if x <= max_mz])\n",
    "    else:\n",
    "        raise ValueError(\"Spectra data must be either a string or a list of floats\")\n",
    "\n",
    "    if len(spectra) > max_peaks:\n",
    "        spectra = spectra[:max_peaks]\n",
    "\n",
    "    bin_indices = np.floor(spectra / bin_precision).astype(int)\n",
    "    encoded = np.zeros(int(max_mz / bin_precision))\n",
    "    encoded[bin_indices] = 1\n",
    "\n",
    "    return encoded\n",
    "\n",
    "# Function for encoding spectra data, vectorized with numpy\n",
    "def encode_mass_spectra_vectorized(spectra, max_mz=500, bin_precision=0.01, max_peaks=100):\n",
    "    # Handle both string and list input\n",
    "    if isinstance(spectra, str):\n",
    "        spectra = np.array([float(x) for x in spectra.strip('[]').split(', ') if float(x) <= max_mz])\n",
    "    elif isinstance(spectra, list):\n",
    "        spectra = np.array(spectra)\n",
    "    else:\n",
    "        raise ValueError(\"Spectra data must be either a string or a list of floats\")\n",
    "\n",
    "    # Ensure spectra are within the specified m/z range\n",
    "    spectra = spectra[spectra <= max_mz]\n",
    "\n",
    "    # Limit the number of peaks\n",
    "    if len(spectra) > max_peaks:\n",
    "        # Sort spectra and select the highest intensity peaks (assuming higher intensity means more important)\n",
    "        spectra = np.sort(spectra)[-max_peaks:]\n",
    "\n",
    "    # Binning procedure\n",
    "    bin_indices = np.floor(spectra / bin_precision).astype(int)\n",
    "    bin_indices = bin_indices[bin_indices < int(max_mz / bin_precision)] # Ensure indices are within range\n",
    "\n",
    "    # Create a one-hot encoded vector for each peak\n",
    "    encoded = np.zeros(int(max_mz / bin_precision))\n",
    "    np.put(encoded, bin_indices, 1)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "# Function for encoding spectra data, using GPU\n",
    "def encode_mass_spectra_gpu(spectra, max_mz=500, bin_precision=0.05, max_peaks=50, device='cuda'):\n",
    "    if isinstance(spectra, str):\n",
    "        spectra = [float(x) for x in spectra.strip('[]').split(', ') if float(x) <= max_mz]\n",
    "    elif not isinstance(spectra, list):\n",
    "        raise ValueError(\"Spectra data must be either a string or a list of floats\")\n",
    "\n",
    "    # Convert to tensor and use half-precision floats\n",
    "    spectra = torch.tensor(spectra, device=device, dtype=torch.float16)\n",
    "\n",
    "    spectra = spectra[spectra <= max_mz]\n",
    "\n",
    "    if spectra.size(0) > max_peaks:\n",
    "        spectra, _ = torch.sort(spectra, descending=True)\n",
    "        spectra = spectra[:max_peaks]\n",
    "\n",
    "    bin_indices = torch.floor(spectra / bin_precision).type(torch.int64)  # int64 for indices\n",
    "    bin_indices = bin_indices[bin_indices < int(max_mz / bin_precision)]\n",
    "\n",
    "    # Use a half-precision float tensor for the encoded array\n",
    "    encoded = torch.zeros(int(max_mz / bin_precision), device=device, dtype=torch.float16)\n",
    "    encoded.scatter_(0, bin_indices, 1)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "\n",
    "# Function for processing each chunk, with garbage collection\n",
    "def process_chunks(chunks, smiles_col, spectra_col, tokenizer):\n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # Create a copy of the chunk to avoid SettingWithCopyWarning\n",
    "        working_chunk = chunk.copy()\n",
    "\n",
    "        # Process the data\n",
    "        working_chunk['tokenized_smiles'] = preprocess_smiles(working_chunk[smiles_col], tokenizer)\n",
    "        working_chunk['encoded_spectra'] = working_chunk[spectra_col].apply(encode_mass_spectra_vectorized)\n",
    "\n",
    "        # Append the processed chunk to the list\n",
    "        processed_chunks.append(working_chunk)\n",
    "\n",
    "        # Free up memory\n",
    "        del working_chunk\n",
    "        gc.collect()\n",
    "        # torch.cuda.empty_cache()  # Optionally clear unused GPU memory after processing\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "# Process training and testing data\n",
    "processed_train_chunks = process_chunks(train_chunks, 'smiles', 'METFRAG_MZ', spe)\n",
    "processed_test_chunks = process_chunks(test_chunks, 'smiles', 'spectrum', spe)\n",
    "\n",
    "print(\"Done! ðŸ™Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e896a-df7c-4a96-9a95-f7b9e70217c3",
   "metadata": {},
   "source": [
    "Concatenate Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3533ab4-6e0e-41e9-86b1-f72f1181f759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:32:11.200619Z",
     "iopub.status.busy": "2023-11-27T21:32:11.199516Z",
     "iopub.status.idle": "2023-11-27T21:32:11.248511Z",
     "shell.execute_reply": "2023-11-27T21:32:11.247589Z",
     "shell.execute_reply.started": "2023-11-27T21:32:11.200589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  smiles  \\\n",
      "0                  Cc1nnc2n1CCC[C@H]2NC(=O)c3cc(cn3C)C#N   \n",
      "1      Cc1cc(sc1C(=O)N2CCC[C@@H](C2)NS(=O)(=O)C)NC(=O...   \n",
      "2      Cc1c(nc(s1)NC(=O)C[C@H]2c3ccccc3C(=O)O2)c4ccc(...   \n",
      "3      COc1ccc2c(c1)CCCN2C(=O)c3ccc(cc3)CS(=O)(=O)c4c...   \n",
      "4        Cc1ccccc1Cc2nc(no2)C[NH+]3CCC4(CC3)OC[C@H](O4)C   \n",
      "...                                                  ...   \n",
      "49995          Cc1cc(cc(c1NC(=O)C)C)NC(=O)c2cccc3c2ccn3C   \n",
      "49996         Cc1ccccc1CN(C)C(=O)N[C@@H](C)C(=O)N2CCCCC2   \n",
      "49997          Cc1cccc(c1)N2CCN(CC2)C(=O)c3cc(nn3C)C(C)C   \n",
      "49998  Cc1ccc(cc1)OCCN2c3ccccc3[C@@](C2=O)(CC(=O)c4cc...   \n",
      "49999            Cc1ccc(cc1)c2csc3c2c(=O)n(cn3)Cc4cccnc4   \n",
      "\n",
      "                                              METFRAG_MZ  \\\n",
      "0      [81.03215, 85.05224, 91.02907, 93.02091, 95.04...   \n",
      "1      [81.987175, 84.08082, 85.052246, 85.08865, 86....   \n",
      "2      [82.98242, 87.03149, 90.046425, 93.03351, 97.9...   \n",
      "3      [85.05224, 90.04643, 91.05426, 92.02567, 104.0...   \n",
      "4      [83.02399, 83.023994, 84.080826, 86.03625, 86....   \n",
      "...                                                  ...   \n",
      "49995  [90.046425, 105.05733, 116.04949, 120.06823, 1...   \n",
      "49996  [85.01583, 85.05224, 85.052246, 86.04748, 87.0...   \n",
      "49997  [81.04474, 85.05224, 85.076065, 91.041664, 94....   \n",
      "49998  [87.03149, 91.041664, 100.0155, 106.041336, 11...   \n",
      "49999  [81.98718, 84.03182, 107.06039, 108.99808, 109...   \n",
      "\n",
      "                                        tokenized_smiles  \\\n",
      "0           Cc1n nc2n1 CCC[C@H]2 NC(=O) c3cc( cn 3C) C#N   \n",
      "1      Cc1cc( sc1 C(=O)N2 CCC[C@@H]( C2) NS(=O)(=O) C...   \n",
      "2      Cc1c( nc( s1) NC(=O) C[C@H]2 c3ccccc3 C(=O)O 2...   \n",
      "3      COc1ccc2c(c1) CCCN2C(=O) c3ccc( cc3) CS(=O)(=O...   \n",
      "4      Cc1ccccc1 C c2nc( no2) C [NH+] 3 CCC4 ( CC3) O...   \n",
      "...                                                  ...   \n",
      "49995  Cc1cc( cc( c1 NC(=O) C)C) NC(=O) c2cccc3 c2cc ...   \n",
      "49996     Cc1ccccc1 CN(C) C(=O)N[C@@H](C) C(=O)N2 CCCCC2   \n",
      "49997  Cc1cccc( c1) N2CCN( CC2) C(=O) c3cc( nn3 C) C(C)C   \n",
      "49998  Cc1ccc( cc1) OCC N2 c3ccccc3 [C@@]( C2=O) ( CC...   \n",
      "49999   Cc1ccc( cc1) c2cs c3 c2c(=O) n( cn3) C c4cccn c4   \n",
      "\n",
      "                                         encoded_spectra  \n",
      "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "...                                                  ...  \n",
      "49995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "49996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "49997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "49998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "49999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "\n",
      "[50000 rows x 4 columns]\n",
      "Done! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "# Concatenate processed training data chunks\n",
    "processed_train_df = pd.concat(processed_train_chunks, ignore_index=True)\n",
    "\n",
    "# Concatenate processed testing data chunks\n",
    "processed_test_df = pd.concat(processed_test_chunks, ignore_index=True)\n",
    "\n",
    "print(processed_train_df)\n",
    "print(\"Done! ðŸ™Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a6f75-caac-4c31-b383-289e38e4bf06",
   "metadata": {},
   "source": [
    "Tranformer and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1056c5-2134-492a-98de-137622893faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:32:11.250315Z",
     "iopub.status.busy": "2023-11-27T21:32:11.249677Z",
     "iopub.status.idle": "2023-11-27T21:32:11.261148Z",
     "shell.execute_reply": "2023-11-27T21:32:11.260357Z",
     "shell.execute_reply.started": "2023-11-27T21:32:11.250289Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp=1024, nhead=16, nhid=4096, nlayers=12, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.transformer = nn.Transformer(d_model=ninp, nhead=nhead, num_encoder_layers=nlayers,\n",
    "                                         num_decoder_layers=nlayers, dim_feedforward=nhid, dropout=dropout)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, \n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        tgt = self.encoder(tgt) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask, memory_mask,\n",
    "                                  src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea94863-344f-45a4-86d6-8c6e65411627",
   "metadata": {},
   "source": [
    "Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfcd58-13a1-4421-ad06-0c2240461fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:32:11.262488Z",
     "iopub.status.busy": "2023-11-27T21:32:11.262269Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import gc\n",
    "\n",
    "# Build Vocabulary from the SPE_ChEMBL.txt file without stripping inner whitespace\n",
    "def build_vocab_from_file(file_path):\n",
    "    vocab = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            token = line.strip('\\n')  # Only strip the newline character\n",
    "            vocab[token] = idx + 1  # Assigning a unique integer to each token\n",
    "    vocab['<pad>'] = 0  # Adding a special token for padding\n",
    "    return vocab\n",
    "\n",
    "# Function to convert tokenized SMILES to integer sequences\n",
    "def smiles_to_integers(tokenized_smiles, vocab):\n",
    "    return [[vocab.get(token, vocab['<pad>']) for token in smile.split()] for smile in tokenized_smiles]\n",
    "\n",
    "# Function to convert tokenized SMILES to tensors in batches\n",
    "def process_smiles_to_tensors(smiles_data, vocab, batch_size=100):\n",
    "    smiles_int_batches = []\n",
    "    max_len = 0\n",
    "    for i in range(0, len(smiles_data), batch_size):\n",
    "        batch_smiles = smiles_data[i:i + batch_size]\n",
    "        smiles_int = smiles_to_integers(batch_smiles, vocab)\n",
    "        max_len_batch = max(len(smile) for smile in smiles_int)\n",
    "        max_len = max(max_len, max_len_batch)\n",
    "        smiles_int_batches.extend(smiles_int)\n",
    "\n",
    "    # Manually pad each sequence to the same length after concatenating\n",
    "    padded_smiles = [item + [vocab['<pad>']] * (max_len - len(item)) for item in smiles_int_batches]\n",
    "    smiles_tensors = torch.tensor(padded_smiles, dtype=torch.long)\n",
    "    return smiles_tensors\n",
    "\n",
    "\n",
    "# Rebuild the vocabulary\n",
    "vocab = build_vocab_from_file('SPE_ChEMBL.txt')\n",
    "\n",
    "# Process tokenized SMILES data in batches\n",
    "train_smiles_tensors = process_smiles_to_tensors(processed_train_df['tokenized_smiles'], vocab)\n",
    "test_smiles_tensors = process_smiles_to_tensors(processed_test_df['tokenized_smiles'], vocab)\n",
    "\n",
    "# Convert encoded spectra (arrays of floats) to tensors and pad\n",
    "train_spectra_tensors = pad_sequence(\n",
    "    [torch.tensor(item, dtype=torch.float16) for item in processed_train_df['encoded_spectra']], \n",
    "    batch_first=True, padding_value=0\n",
    ")\n",
    "test_spectra_tensors = pad_sequence(\n",
    "    [torch.tensor(item, dtype=torch.float16) for item in processed_test_df['encoded_spectra']], \n",
    "    batch_first=True, padding_value=0\n",
    ")\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # If using GPU\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_smiles_tensors, train_spectra_tensors)\n",
    "test_dataset = TensorDataset(test_smiles_tensors, test_spectra_tensors)\n",
    "\n",
    "# Move tensors to GPU in data loaders (if available)\n",
    "def collate_fn(batch):\n",
    "    smiles, spectra = zip(*batch)\n",
    "    return torch.stack(smiles).to(device), torch.stack(spectra).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Done ðŸ™Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76737f79-82a5-4de2-a50c-98084a353826",
   "metadata": {},
   "source": [
    "Define model and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e79145-d4f5-4fe2-b016-e8199a4a3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, loss function, optimizer\n",
    "ntokens = 3002 # Size of vocabulary (from SPE Vocab list)\n",
    "model = TransformerModel(ntokens).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10 # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for smiles, spectra in train_loader:\n",
    "        smiles, spectra = smiles.to(device), spectra.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(smiles, spectra)\n",
    "        loss = criterion(output.view(-1, ntokens), spectra.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Logging\n",
    "    print(f'Epoch: {epoch}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for smiles, spectra in test_loader:\n",
    "            smiles, spectra = smiles.to(device), spectra.to(device)\n",
    "            output = model(smiles, spectra)\n",
    "            total_loss += criterion(output.view(-1, ntokens), spectra.view(-1)).item()\n",
    "    print(f'Validation Loss: {total_loss / len(test_loader)}')\n",
    "\n",
    "    # Memory Clearing\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Done ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c90f42-7c7b-43a8-9458-cc6346b602de",
   "metadata": {},
   "source": [
    "View model's output on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df74f17-5380-4b32-8815-c3cc8fd28811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_output(model, data_loader, num_samples=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (smiles, spectra) in enumerate(data_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            smiles, spectra = smiles.to(device), spectra.to(device)\n",
    "            output = model(smiles, spectra)\n",
    "            \n",
    "            # Assuming the output is in the same format as your encoded spectra\n",
    "            # Convert output tensor to numpy array for display\n",
    "            output_np = output.cpu().numpy()\n",
    "            spectra_np = spectra.cpu().numpy()\n",
    "            smiles_np = smiles.cpu().numpy()  # Convert if necessary\n",
    "\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(\"Input SMILES: \", smiles_np)\n",
    "            print(\"True Spectra: \", spectra_np)\n",
    "            print(\"Predicted Spectra: \", output_np)\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Display model output on validation/testing data\n",
    "display_model_output(model, test_loader)\n",
    "\n",
    "print(\"ðŸ‘€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
